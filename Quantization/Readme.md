# ğŸ§  LLM Quantization Performance Benchmark  

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)  
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)  
![BitsAndBytes](https://img.shields.io/badge/BitsAndBytes-4bit_Quantization-green.svg)  
![CUDA](https://img.shields.io/badge/Accelerated_with-CUDA_12.4-orange.svg)  
![Category](https://img.shields.io/badge/Domain-AI_|_LLM_Optimization-red.svg)  

---

## ğŸ¯ Overview  

This project demonstrates **Quantization of Large Language Models (LLMs)** to achieve major **memory savings** while keeping **language understanding performance** nearly intact.  
It evaluates **Microsoft Phi-3**, **Google Gemma**, and **Qwen 2.5** models using **BitsAndBytes 4-bit quantization** and compares them against their **full-precision (FP16)** counterparts.  

---

## âš™ï¸ Features  

âœ… **Quantized vs. Unquantized comparison** for perplexity (PPL) and memory footprint  
âœ… Uses **4-bit NF4 quantization** via `BitsAndBytesConfig`  
âœ… Benchmarks **3 leading open-source LLMs**  
âœ… Built with **PyTorch**, **Transformers**, and **Accelerate**  
âœ… Visualized results using **pandas styled DataFrame**   

---

## ğŸ§© Tech Stack  

| Category | Tools / Frameworks |
|-----------|--------------------|
| **Language** | Python 3.10+ |
| **Libraries** | Transformers, BitsAndBytes, Torch, Accelerate, Pandas, Matplotlib |
| **Models Tested** | `microsoft/Phi-3-mini-4k-instruct`, `google/gemma-2-2b-it`, `Qwen/Qwen2.5-3B-Instruct` |
| **Hardware** | CUDA 12.4 (GPU accelerated) |
| **Concepts** | LLM Quantization, Memory Optimization, NLP Evaluation, Model Efficiency |

---

## âš¡ Workflow  

ğŸŒ Load Model â†’ âš™ï¸ Quantize with BitsAndBytes â†’ ğŸ”¡ Compute Perplexity â†’ ğŸ’¾ Measure Memory â†’ ğŸ“Š Compare Results

---

## ğŸ“Š Output Preview  


| Model | Quantized PPL | Unquantized PPL | Quantized Memory (MB) | Unquantized Memory (MB) |
|--------|----------------|----------------|------------------------|--------------------------|
| Phi-3-mini-4k-instruct | **10.44** | **10.25** | **2.2 GB** &nbsp;&nbsp; â¬‡ï¸**71% smaller** | **7.6 GB** |
| gemma-2-2b-it | **16.39** | **15.21** | **2.1 GB** &nbsp;&nbsp; â¬‡ï¸**58% smaller** | **5.2 GB** |
| Qwen2.5-3B-Instruct | **16.28** | **15.39** | **2.0 GB** &nbsp;&nbsp; â¬‡ï¸**67% smaller** | **6.1 GB** |

---

## ğŸ’° Efficiency Insight  


| Metric | Impact |
|---------|---------|
| ğŸ§® **Perplexity (PPL)** | Quantized models show a **minor increase (<1â€“7%)** in PPL compared to full-precision, proving **minimal loss in language quality**. |
| ğŸ’¾ **Memory Reduction** | Quantization reduces memory usage from **~7.6 GB â†’ ~2.2 GB**, achieving up to **65â€“70% GPU memory savings** without architecture changes. |
| âš¡ **Performance Trade-off** | Despite aggressive 4-bit compression, **response quality remains consistent**, making quantized models ideal for **deployment on limited-VRAM GPUs**. |
| ğŸ§  **Model Efficiency** | Demonstrates **efficient scaling across Phi-3, Gemma, and Qwen**, confirming 4-bit NF4 quantization is both **cost-effective** and **production-ready**. |


---

