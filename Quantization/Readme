# üöÄ Large Language Model Quantization Benchmarking

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-red.svg)
![Transformers](https://img.shields.io/badge/ü§ó_Transformers-4.48.3-yellow.svg)
![CUDA](https://img.shields.io/badge/CUDA-12.4-green.svg)
![License](https://img.shields.io/badge/License-MIT-purple.svg)

**Comprehensive analysis of 4-bit quantization techniques for optimizing LLM inference and deployment**

[Overview](#-overview) ‚Ä¢ [Features](#-key-features) ‚Ä¢ [Models](#-models-tested) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Results](#-results)

</div>

---

## üìä Overview

This project demonstrates **advanced model optimization techniques** for Large Language Models (LLMs) using **4-bit quantization** with BitsAndBytes. It provides a systematic comparison of quantized vs. unquantized models, measuring critical metrics like **perplexity** and **memory footprint** to enable efficient **model deployment** and **production-ready inference**.

### üéØ Business Impact
- ‚úÖ **70-75% memory reduction** enabling deployment on consumer GPUs
- ‚úÖ **Cost-effective inference** for production environments
- ‚úÖ **Minimal accuracy degradation** while maintaining model performance
- ‚úÖ **Scalable ML solutions** for resource-constrained environments

---

## üîë Key Features

| Feature | Description |
|---------|-------------|
| üß† **Model Compression** | 4-bit NF4 quantization with double quantization for optimal memory efficiency |
| üìà **Performance Metrics** | Automated perplexity calculation and memory footprint analysis |
| üîÑ **Multi-Model Support** | Benchmarking across Phi-3, Gemma-2, and Qwen2.5 architectures |
| ‚ö° **GPU Optimization** | CUDA-accelerated inference with bfloat16 compute dtype |
| üìä **Visual Analytics** | Styled pandas DataFrames with color-coded performance comparisons |
| üîß **Production-Ready** | Clean resource management with garbage collection and CUDA cache clearing |

---

## ü§ñ Models Tested

This benchmark evaluates three state-of-the-art **instruction-tuned LLMs**:

1. **Microsoft Phi-3-mini-4k-instruct** - Compact yet powerful 3.8B parameter model
2. **Google Gemma-2-2b-it** - Efficient 2B parameter instruction-following model
3. **Qwen2.5-3B-Instruct** - Advanced 3B parameter multilingual model

---

## üõ†Ô∏è Technical Stack

**Deep Learning Frameworks:**
- PyTorch 2.5.1 (CUDA 12.4)
- Hugging Face Transformers 4.48.3
- Accelerate 1.3.0

**Quantization & Optimization:**
- BitsAndBytes 0.46.0 (4-bit quantization)
- NF4 quantization type
- Double quantization enabled
- bfloat16 compute dtype

**Analysis & Visualization:**
- Pandas (structured data analysis)
- Matplotlib (visualization)

---

## üì• Installation

```bash
# Install PyTorch with CUDA 12.4 support
pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 \
    --index-url https://download.pytorch.org/whl/cu124

# Install dependencies for model quantization and inference
pip install requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0
```

### üíª Requirements
- **GPU:** NVIDIA GPU with CUDA 12.4+ support (minimum 8GB VRAM recommended)
- **RAM:** 16GB+ system memory
- **Python:** 3.8 or higher

---

## üöÄ Usage

```python
# Run the quantization benchmark
python Quantization.py
```

The script will:
1. Load each model in both quantized (4-bit) and unquantized (FP16) formats
2. Calculate **perplexity** on sample text to measure model quality
3. Measure **memory footprint** for each configuration
4. Generate a styled comparison table with performance metrics

---

## üìä Results

The benchmark produces a comprehensive comparison table showing:

| Metric | Description | Impact |
|--------|-------------|--------|
| **Quantized PPL** | Perplexity of 4-bit quantized model | Lower is better (model quality) |
| **Unquantized PPL** | Perplexity of FP16 baseline model | Baseline performance reference |
| **Quantized Memory** | Memory usage with 4-bit quantization | ~70-75% reduction vs FP16 |
| **Unquantized Memory** | Memory usage in FP16 format | Full precision baseline |

### üé® Color-Coded Visualization
- üü¢ **Green:** Quantized perplexity metrics
- üü† **Orange:** Memory consumption metrics
- Lighter shades indicate unquantized baseline comparisons

---

## üî¨ Key Concepts

### **Model Quantization**
Reducing model precision from 16-bit (FP16) to 4-bit representation while preserving model accuracy. This enables:
- Efficient **model deployment** on edge devices
- Lower **inference latency** for real-time applications
- Reduced **cloud computing costs**

### **Perplexity (PPL)**
A standard metric for evaluating language model quality. Lower perplexity indicates better model performance. Minimal PPL degradation after quantization demonstrates effective compression.

### **NF4 Quantization**
Normal Float 4-bit quantization optimized for neural network weights, providing superior performance compared to standard 4-bit integers.

---

## üíº Skills Demonstrated

This project showcases expertise in:

- ‚ú® **Machine Learning Engineering** - Production-ready model optimization
- ‚ú® **Deep Learning** - Neural network compression techniques
- ‚ú® **MLOps** - Model deployment and resource optimization
- ‚ú® **Natural Language Processing** - LLM evaluation and benchmarking
- ‚ú® **GPU Computing** - CUDA optimization and memory management
- ‚ú® **Python Development** - Clean, modular, production-grade code
- ‚ú® **Performance Optimization** - Latency and memory footprint reduction
- ‚ú® **Model Deployment** - Efficient inference strategies
- ‚ú® **Data Analysis** - Metrics evaluation and visualization

---

## üîÆ Future Enhancements

- [ ] GGUF quantization support for CPU inference
- [ ] INT8 quantization comparison
- [ ] Inference speed benchmarking
- [ ] Token generation performance metrics
- [ ] Multi-GPU distributed quantization
- [ ] ONNX export for cross-platform deployment
- [ ] Fine-tuning quantized models with PEFT/LoRA

---

## üìö References

- [BitsAndBytes Documentation](https://github.com/TimDettmers/bitsandbytes)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [LLM Quantization Research Papers](https://arxiv.org/abs/2106.09685)

---

## üìÑ License

This project is available under the MIT License.

---

## ü§ù Contributing

Contributions, issues, and feature requests are welcome! Feel free to check the issues page.

---

<div align="center">

### ‚≠ê If you find this project useful, please consider giving it a star!

**Built with ‚ù§Ô∏è for efficient AI deployment**

</div>
